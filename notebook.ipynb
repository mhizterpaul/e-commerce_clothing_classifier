{"cells":[{"cell_type":"markdown","id":"aaa02648-9eae-45ba-893f-88440e8e4235","metadata":{},"source":["![clothing_classification](clothing_classification.png)\n"]},{"cell_type":"markdown","id":"ad5a988c-1095-485a-a88c-002400a872be","metadata":{},"source":["Fashion Forward is a new AI-based e-commerce clothing retailer.\n","They want to use image classification to automatically categorize new product listings, making it easier for customers to find what they're looking for. It will also assist in inventory management by quickly sorting items.\n","\n","As a data scientist tasked with implementing a garment classifier, your primary objective is to develop a machine learning model capable of accurately categorizing images of clothing items into distinct garment types such as shirts, trousers, shoes, etc."]},{"cell_type":"code","execution_count":1,"id":"4a1ab317-f3e4-4e5f-93a7-9c27677c5ffb","metadata":{"executionCancelledAt":null,"executionTime":162,"lastExecutedAt":1734279562240,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run the cells below first"},"outputs":[],"source":["# Run the cells below first"]},{"cell_type":"code","execution_count":2,"id":"93e7dae3-c192-4267-a0ed-18d1ac56c861","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":6889,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1734279569129,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmetrics\n!pip install torchvision","outputsMetadata":{"0":{"height":544,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.9.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\n","Downloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.11.9 torchmetrics-1.5.2\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.2.0)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.7.99)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision) (65.6.3)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision) (0.38.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision) (1.25.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install torchmetrics\n","!pip install torchvision"]},{"cell_type":"code","execution_count":3,"id":"ea8065b7-84fc-4376-afef-6db731dec4b3","metadata":{"executionCancelledAt":null,"executionTime":5587,"lastExecutedAt":1734279574718,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchmetrics import Accuracy, Precision, Recall"]},{"cell_type":"code","execution_count":4,"id":"662e1bf1-943f-4243-9fd4-02ce11609e8d","metadata":{"executionCancelledAt":null,"executionTime":413,"lastExecutedAt":1734279575132,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ndef to_rgb(image):\n    return image.convert(\"RGB\")\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n    transforms.Lambda(to_rgb),\n    #transforms.RandomVerticalFlip(p=0.5),\n    #transforms.RandomRotation(90),\n    #transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n                                  )\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n    transforms.Lambda(to_rgb),\n    transforms.Resize((64,64)),\n    transforms.ToTensor()\n])\n                                 )","outputsMetadata":{"0":{"height":80,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":122,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":122,"type":"stream"},"5":{"height":38,"type":"stream"},"6":{"height":122,"type":"stream"},"7":{"height":38,"type":"stream"},"8":{"height":59,"type":"stream"}}},"outputs":[],"source":["# Load datasets\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","\n","def to_rgb(image):\n","    return image.convert(\"RGB\")\n","\n","train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n","    transforms.Lambda(to_rgb),\n","    #transforms.RandomVerticalFlip(p=0.5),\n","    #transforms.RandomRotation(90),\n","    #transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n","    transforms.Resize((64, 64)),\n","    transforms.ToTensor()\n","])\n","                                  )\n","test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n","    transforms.Lambda(to_rgb),\n","    transforms.Resize((64,64)),\n","    transforms.ToTensor()\n","])\n","                                 )"]},{"cell_type":"code","execution_count":5,"id":"53c0a71d-d7d9-4a11-8a9b-55867ea7e0b5","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1734279575187,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#model architecture\nclass MultiClassImageClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(MultiClassImageClassifier, self).__init__()\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n            nn.ELU(),\n            nn.MaxPool2d(kernel_size=4),\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.ELU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Flatten()  \n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(32*8*8, num_classes),\n            nn.Softmax(dim=-1)\n        )\n    def forward(self,x):\n        x = self.feature_extractor(x)\n        x = self.classifier(x)\n        return x\n            "},"outputs":[],"source":["#model architecture\n","class MultiClassImageClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(MultiClassImageClassifier, self).__init__()\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n","            nn.ELU(),\n","            nn.MaxPool2d(kernel_size=4, stride=4),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n","            nn.ELU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Flatten()  \n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(32*8*8, num_classes),\n","            nn.Softmax(dim=-1)\n","        )\n","    def forward(self,x):\n","        x = self.feature_extractor(x)\n","        x = self.classifier(x)\n","        return x\n","            "]},{"cell_type":"code","execution_count":6,"id":"919afdbc-78fd-4ddc-a4f9-ce1b267ff62a","metadata":{"executionCancelledAt":null,"executionTime":57,"lastExecutedAt":1734279575245,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get the number of classes\n\nclasses = train_data.classes \nprint(classes) \nnum_classes = len(train_data.classes) \nprint(num_classes)","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","10\n"]}],"source":["# Get the number of classes\n","\n","classes = train_data.classes \n","print(classes) \n","num_classes = len(train_data.classes) \n","print(num_classes)"]},{"cell_type":"code","execution_count":7,"id":"ba4b189f-1171-45f5-b295-c4fab0e9bc5f","metadata":{"executionCancelledAt":null,"executionTime":188443,"lastExecutedAt":1734279763688,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"\ndataloader_train = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\nnum_classes = 10\n#defind the model\nmodel = MultiClassImageClassifier(num_classes=num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nepoch_loss = []\nrunning_loss = 0.0\nmodel.train()\nfor epoch in range(2):\n    for images, labels in dataloader_train:\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    epoch_loss.append(running_loss / len(dataloader_train))\n    running_loss = 0.0\n\nprint(epoch_loss)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.6446020466486613, 1.600796412118276]\n"]}],"source":["\n","dataloader_train = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n","num_classes = 10\n","#defind the model\n","model = MultiClassImageClassifier(num_classes=num_classes)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","epoch_loss = []\n","running_loss = 0.0\n","model.train()\n","for epoch in range(2):\n","    for images, labels in dataloader_train:\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    epoch_loss.append(running_loss / len(dataloader_train))\n","    running_loss = 0.0\n","\n","print(epoch_loss)"]},{"cell_type":"code","execution_count":8,"id":"b50be612-c4a3-4ed6-b635-7d1fd8b54687","metadata":{"executionCancelledAt":null,"executionTime":16395,"lastExecutedAt":1734279780084,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"\ndataloader_test = DataLoader(dataset=test_data, batch_size=16, shuffle=True)\n\nmetric_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\nmetric_precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None)\nmetric_recall = Recall(task=\"multiclass\", num_classes=num_classes, average=None)\n\npredictions = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in dataloader_test:\n        outputs = model.forward(images)\n        preds = torch.argmax(outputs, dim=-1)\n        predictions.extend(preds.tolist())\n        metric_accuracy(preds, labels)\n        metric_precision(preds, labels)\n        metric_recall(preds, labels)\n        \naccuracy = metric_accuracy.compute().item()\nprint(f\"accuracy: {accuracy}\")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy: 0.8622999787330627\n"]}],"source":["\n","dataloader_test = DataLoader(dataset=test_data, batch_size=16, shuffle=True)\n","\n","metric_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","metric_precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None)\n","metric_recall = Recall(task=\"multiclass\", num_classes=num_classes, average=None)\n","\n","predictions = []\n","\n","model.eval()\n","with torch.no_grad():\n","    for images, labels in dataloader_test:\n","        outputs = model.forward(images)\n","        preds = torch.argmax(outputs, dim=-1)\n","        predictions.extend(preds.tolist())\n","        metric_accuracy(preds, labels)\n","        metric_precision(preds, labels)\n","        metric_recall(preds, labels)\n","        \n","accuracy = metric_accuracy.compute().item()\n","print(f\"accuracy: {accuracy}\")"]},{"cell_type":"code","execution_count":9,"id":"509f4cfc-005a-426d-bc96-bff6f6e8b488","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1734279780136,"lastExecutedByKernel":"4d21d79d-fa64-489c-aea4-95bbe00a6941","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"precision = metric_precision.compute()\nrecall = metric_recall.compute()\n\nprecision_per_class = {\n    k: precision[v].item()\n    for k, v \n    in test_data.class_to_idx.items()\n}\nprint(f\"precison per class:\\n{precision_per_class}\\n\")\n\n\nrecall_per_class = {\n    k: recall[v].item()\n    for k, v \n    in test_data.class_to_idx.items()\n}\nprint(f\"recall pers class:\\n{recall_per_class}\")","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["precison per class:\n","{'T-shirt/top': 0.8178178071975708, 'Trouser': 0.9795709848403931, 'Pullover': 0.7798253893852234, 'Dress': 0.7943686246871948, 'Coat': 0.813238799571991, 'Sandal': 0.9813277721405029, 'Shirt': 0.6270492076873779, 'Sneaker': 0.9297561049461365, 'Bag': 0.9625884890556335, 'Ankle boot': 0.9430814385414124}\n","\n","recall pers class:\n","{'T-shirt/top': 0.8169999718666077, 'Trouser': 0.9589999914169312, 'Pullover': 0.8040000200271606, 'Dress': 0.9309999942779541, 'Coat': 0.6880000233650208, 'Sandal': 0.9459999799728394, 'Shirt': 0.6119999885559082, 'Sneaker': 0.953000009059906, 'Bag': 0.9520000219345093, 'Ankle boot': 0.9610000252723694}\n"]}],"source":["precision = metric_precision.compute()\n","recall = metric_recall.compute()\n","\n","precision_per_class = {\n","    k: precision[v].item()\n","    for k, v \n","    in test_data.class_to_idx.items()\n","}\n","print(f\"precison per class:\\n{precision_per_class}\\n\")\n","\n","\n","recall_per_class = {\n","    k: recall[v].item()\n","    for k, v \n","    in test_data.class_to_idx.items()\n","}\n","print(f\"recall pers class:\\n{recall_per_class}\")"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
